{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre Classification \n",
    "This is an example for document Genre classification using the Brown corpus (provided by NLTK).\n",
    "\n",
    "The 5000 most common words are taken from the Brown Corpus. Then we create a 5000 dimensional bag of words and input this to a neural network. The network predicts the genre.\n",
    "\n",
    "The input for the network is a 5000 dimensional vector. Each position correspondence to one of the most common words. The value is set to 1, if the word appears in a document. Otherwise to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus\n",
    "Reads in the corpus and create a bag of word representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import nltk.corpus\n",
    "import random\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_max_words = 5000\n",
    "\n",
    "stopwords = {}\n",
    "for stopword in nltk.corpus.stopwords.words('english'):\n",
    "    stopwords[stopword.lower()] = True\n",
    "    \n",
    "\n",
    "def preprocessDocument(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w.lower()) for w in words if len(w) >= 3 and w.lower() not in stopwords]\n",
    "\n",
    "brown_words = brown.words()\n",
    "fdist = nltk.FreqDist(preprocessDocument(brown_words))\n",
    "\n",
    "\n",
    "max_words = []\n",
    "for word, freq in fdist.most_common(num_max_words):\n",
    "    max_words.append(word)\n",
    "    \n",
    "max_words = sorted(max_words)   \n",
    "\n",
    "max_words_idx = {}\n",
    "idx = 0\n",
    "\n",
    "for max_word in max_words:\n",
    "    max_words_idx[max_word] = idx\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "\n",
    "def getBoW(words):\n",
    "    outputvector = [0]*len(max_words)\n",
    "    \n",
    "    prepocessed = preprocessDocument(words)\n",
    "    \n",
    "    for word in prepocessed:\n",
    "        if word in max_words_idx:\n",
    "            idx = max_words_idx[word]\n",
    "            outputvector[idx] = 1 \n",
    "    \n",
    "    return outputvector\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Set\n",
    "This creates the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File IDs: ca01,ca02,ca03,ca04,ca05,ca06,ca07,ca08,ca09,ca10\n",
      "Train File IDs: ce29,ce05,ck08,cg07,ck26,cj05,cf07,cg65,cj70,cj11\n",
      "Test File IDs: cn11,cg73,cp18,cf40,ca11,ca09,cf21,cj67,cg51,ch13\n"
     ]
    }
   ],
   "source": [
    "category2Idx = {}\n",
    "idx = 0\n",
    "for cat in brown.categories():\n",
    "    category2Idx[cat] = idx\n",
    "    idx += 1\n",
    "\n",
    "file_ids = sorted(brown.fileids())\n",
    "print \"File IDs:\",\",\".join(file_ids[0:10])\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(file_ids)\n",
    "\n",
    "train_file_ids, test_file_ids = file_ids[0:300],file_ids[300:]\n",
    "\n",
    "print \"Train File IDs:\",\",\".join(train_file_ids[0:10])\n",
    "print \"Test File IDs:\",\",\".join(test_file_ids[0:10])\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for fileid in train_file_ids:\n",
    "    category = brown.categories(fileid)[0]\n",
    "    all_words = brown.words(fileid) \n",
    "    bow = getBoW(all_words)\n",
    "    \n",
    "    train_x.append(bow)\n",
    "    train_y.append(category2Idx[category])\n",
    "\n",
    "for fileid in test_file_ids:\n",
    "    category = brown.categories(fileid)[0]\n",
    "    all_words = brown.words(fileid) \n",
    "    bow = getBoW(all_words)\n",
    "    \n",
    "    test_x.append(bow)\n",
    "    test_y.append(category2Idx[category])\n",
    "    \n",
    "\n",
    "train_x = np.asarray(train_x, dtype='int32')\n",
    "train_y = np.asarray(train_y, dtype='int32')\n",
    "test_x = np.asarray(test_x, dtype='int32')\n",
    "test_y = np.asarray(test_y, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Given the Training and Test sets, we now define a feed forward network. We use a 500 dimensional hidden layer with dropout of 0.5.\n",
    "\n",
    "Feel free to try different hidden layer sizes and number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test score before fine turning:', 2.7576393604278566)\n",
      "('Test accuracy before fine turning:', 0.070000000000000007)\n",
      "Train on 300 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 0s - loss: 2.3795 - acc: 0.2833 - val_loss: 2.0335 - val_acc: 0.3100\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 0.8440 - acc: 0.8200 - val_loss: 1.6403 - val_acc: 0.4200\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 0.3087 - acc: 0.9833 - val_loss: 1.4993 - val_acc: 0.4400\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 1.4861 - val_acc: 0.4500\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 1.4593 - val_acc: 0.4750\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 0.0313 - acc: 1.0000 - val_loss: 1.4691 - val_acc: 0.4450\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 0.0243 - acc: 1.0000 - val_loss: 1.4730 - val_acc: 0.4400\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 0.0176 - acc: 1.0000 - val_loss: 1.4788 - val_acc: 0.4450\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 0.0155 - acc: 1.0000 - val_loss: 1.4782 - val_acc: 0.4750\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 0.0133 - acc: 1.0000 - val_loss: 1.4798 - val_acc: 0.4650\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 0.0118 - acc: 1.0000 - val_loss: 1.4776 - val_acc: 0.4700\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 0.0113 - acc: 1.0000 - val_loss: 1.4777 - val_acc: 0.4700\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 0.0088 - acc: 1.0000 - val_loss: 1.4795 - val_acc: 0.4750\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 1.4851 - val_acc: 0.4650\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 1.4893 - val_acc: 0.4600\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 1.4878 - val_acc: 0.4700\n",
      "Epoch 17/50\n",
      "270/300 [==========================>...] - ETA: 0s - loss: 0.0065 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e243ce4ce096>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy before fine turning:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m model.fit(train_x, train_y_cat, batch_size=batch_size, nb_epoch=nb_epoch,\n\u001b[1;32m---> 27\u001b[1;33m           show_accuracy=True, validation_data=(test_x, test_y_cat))\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score after fine turning:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Keras-0.2.0-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    487\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Keras-0.2.0-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[0;32m    221\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                         \u001b[1;31m# replace with self._evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                         \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                             \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Keras-0.2.0-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/likewise-open/UKP/reimers/NLP/Tools/Theano/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import containers\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, AutoEncoder, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 30\n",
    "nb_epoch = 50\n",
    "nb_classes = len(category2Idx)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=num_max_words, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "train_y_cat = np_utils.to_categorical(train_y, nb_classes)\n",
    "test_y_cat = np_utils.to_categorical(test_y, nb_classes)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "score = model.evaluate(test_x, test_y_cat, show_accuracy=True, verbose=0)\n",
    "print('Test score before fine turning:', score[0])\n",
    "print('Test accuracy before fine turning:', score[1])\n",
    "model.fit(train_x, train_y_cat, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, validation_data=(test_x, test_y_cat))\n",
    "score = model.evaluate(test_x, test_y_cat, show_accuracy=True, verbose=0)\n",
    "print('Test score after fine turning:', score[0])\n",
    "print('Test accuracy after fine turning:', score[1])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
