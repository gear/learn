{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder and Deep Neural Networks\n",
    "This scripts reads in the 20 newsgroup corpus from SKLearn. Each document is created to a BoW-vector over the 2000 most common words.\n",
    "\n",
    "1) Computes a baseline using Naive Bayes and SVM\n",
    "\n",
    "2) Deep Feed Forward Network\n",
    "\n",
    "3) AutoEncoder (should work in principle, but it does not yet converge. Check the parameters and the training method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "max_words = 2000\n",
    "examples_per_labels = 1000\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "#count_vect = CountVectorizer(stop_words='english', max_features=max_words)\n",
    "count_vect = TfidfVectorizer(stop_words='english', max_features=max_words)\n",
    "train_x = count_vect.fit_transform(newsgroups_train.data).toarray()\n",
    "test_x = count_vect.transform(newsgroups_test.data).toarray()\n",
    "\n",
    "train_y = newsgroups_train.target\n",
    "test_y = newsgroups_test.target\n",
    "\n",
    "nb_labels = max(train_y)+1\n",
    "\n",
    "print \"Train: \",train_x.shape\n",
    "print \"Test: \",test_x.shape\n",
    "print \"%d labels\" % nb_labels\n",
    "\n",
    "\n",
    "\n",
    "examples = []\n",
    "examples_labels = []\n",
    "examples_count = {}\n",
    "\n",
    "for idx in xrange(train_x.shape[0]):\n",
    "    label = train_y[idx]\n",
    "    \n",
    "    if label not in examples_count:\n",
    "        examples_count[label] = 0\n",
    "    \n",
    "    if examples_count[label] < examples_per_labels:\n",
    "        arr = train_x[idx]\n",
    "        examples.append(arr)\n",
    "        examples_labels.append(label)\n",
    "        examples_count[label]+=1\n",
    "\n",
    "train_subset_x = np.asarray(examples)\n",
    "train_subset_y = np.asarray(examples_labels)\n",
    "\n",
    "print \"Train Subset: \",train_subset_x.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "We use Scikit-learn to derive some baselines for the above setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB(alpha=.01)\n",
    "clf.fit(train_subset_x, train_subset_y)\n",
    "pred = clf.predict(test_x)\n",
    "acc = metrics.accuracy_score(test_y, pred)\n",
    "print \"Naive Bayes: %f%%\" % (acc*100)\n",
    "\n",
    "#Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_subset_x, train_subset_y)\n",
    "pred = clf.predict(test_x)\n",
    "acc = metrics.accuracy_score(test_y, pred)\n",
    "print \"Gaussian Naive Bayes: %f%%\" % (acc*100)\n",
    "\n",
    "#MultinomialNB \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_subset_x, train_subset_y)  \n",
    "pred = clf.predict(test_x)\n",
    "acc = metrics.accuracy_score(test_y, pred)\n",
    "print \"Multinomial Naive Bayes: %f%%\" % (acc*100)\n",
    "\n",
    "#LinearSVM\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(train_subset_x, train_subset_y)  \n",
    "pred = clf.predict(test_x)\n",
    "acc = metrics.accuracy_score(test_y, pred)\n",
    "print \"LinearSVM: %f%%\" % (acc*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import containers\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, AutoEncoder, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "nb_epoch = 30\n",
    "batch_size = 200\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=max_words, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(500, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_labels, activation='softmax'))\n",
    "\n",
    "train_subset_y_cat = np_utils.to_categorical(train_subset_y, nb_labels)\n",
    "test_y_cat = np_utils.to_categorical(test_y, nb_labels)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "print('Start training')\n",
    "model.fit(train_subset_x, train_subset_y_cat, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, verbose=True, validation_data=(test_x, test_y_cat))\n",
    "\n",
    "score = model.evaluate(test_x, test_y_cat, show_accuracy=True, verbose=False)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "This is the code how the autoencoder should work in principle. However, the pretraining seems not to work as the loss stays approx. identical for all epochs. If someone finds the problem, please send me an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "# Source: https://github.com/fchollet/keras/issues/358\n",
    "from keras.layers import containers\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, AutoEncoder, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "random.seed(3)\n",
    "np.random.seed(3)\n",
    "\n",
    "nb_epoch = 30\n",
    "batch_size = 200\n",
    "\n",
    "nb_epoch_pretraining = 10\n",
    "batch_size_pretraining = 1000\n",
    "\n",
    "\n",
    "# Layer-wise pretraining\n",
    "encoders = []\n",
    "decoders = []\n",
    "nb_hidden_layers = [max_words, 1000]\n",
    "X_train_tmp = np.copy(train_x)\n",
    "for i, (n_in, n_out) in enumerate(zip(nb_hidden_layers[:-1], nb_hidden_layers[1:]), start=1):\n",
    "    print('Training the layer {}: Input {} -> Output {}'.format(i, n_in, n_out))\n",
    "    # Create AE and training\n",
    "    ae = Sequential()\n",
    "    encoder = containers.Sequential([Dense(output_dim=n_out, input_dim=n_in, activation='tanh'), Dropout(0.3)])\n",
    "    decoder = containers.Sequential([Dense(output_dim=n_in, input_dim=n_out, activation='tanh')])\n",
    "    ae.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))\n",
    "    \n",
    "    sgd = SGD(lr=2, decay=1e-6, momentum=0.0, nesterov=True)\n",
    "    ae.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "    ae.fit(X_train_tmp, X_train_tmp, batch_size=batch_size_pretraining, nb_epoch=nb_epoch_pretraining, verbose = True, shuffle=True)\n",
    "    # Store trainined weight and update training data\n",
    "    encoders.append(ae.layers[0].encoder)\n",
    "    decoders.append(ae.layers[0].decoder)\n",
    "    X_train_tmp = ae.predict(X_train_tmp)\n",
    "    \n",
    "#End to End Autoencoder training    \n",
    "if len(nb_hidden_layers) > 2:\n",
    "    full_encoder = containers.Sequential()\n",
    "    for encoder in encoders:\n",
    "        full_encoder.add(encoder)\n",
    "\n",
    "    full_decoder = containers.Sequential()\n",
    "    for decoder in reversed(decoders):\n",
    "        full_decoder.add(decoder)\n",
    "\n",
    "    full_ae = Sequential()\n",
    "    full_ae.add(AutoEncoder(encoder=full_encoder, decoder=full_decoder, output_reconstruction=False))    \n",
    "    full_ae.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "\n",
    "    print \"Pretraining of full AE\"\n",
    "    full_ae.fit(train_x, train_x, batch_size=batch_size_pretraining, nb_epoch=nb_epoch_pretraining, verbose = True, shuffle=True)\n",
    "\n",
    "    \n",
    "# Fine-turning\n",
    "model = Sequential()\n",
    "for encoder in encoders:\n",
    "    model.add(encoder)\n",
    "    \n",
    "model.add(Dense(output_dim=nb_labels, input_dim=nb_hidden_layers[-1], activation='softmax'))\n",
    "\n",
    "train_subset_y_cat = np_utils.to_categorical(train_subset_y, nb_labels)\n",
    "test_y_cat = np_utils.to_categorical(test_y, nb_labels)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "score = model.evaluate(test_x, test_y_cat, show_accuracy=True, verbose=0)\n",
    "print('Test score before fine turning:', score[0])\n",
    "print('Test accuracy before fine turning:', score[1])\n",
    "model.fit(train_subset_x, train_subset_y_cat, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, validation_data=(test_x, test_y_cat), shuffle=True)\n",
    "score = model.evaluate(test_x, test_y_cat, show_accuracy=True, verbose=0)\n",
    "print('Test score after fine turning:', score[0])\n",
    "print('Test accuracy after fine turning:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
